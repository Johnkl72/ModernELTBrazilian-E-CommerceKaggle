# üáßüá∑ Modern ELT Lakehouse - Brazilian E-Commerce (Olist)

Este proyecto implementa un pipeline de Ingenier√≠a de Datos End-to-End de nivel Enterprise utilizando la arquitectura Lakehouse en Databricks.

El objetivo es transformar datos crudos de e-commerce (Dataset Olist de Kaggle) en un Modelo Dimensional (Star Schema) optimizado para anal√≠tica de negocio, aplicando mejores pr√°cticas como SCD Type 1, Optimizaci√≥n Z-Order, Manejo de Esquemas y Calidad de Datos.

## üèóÔ∏è Arquitectura del Pipeline (Medallion)

El flujo de datos sigue la arquitectura Multi-hop (Bronze ‚Üí Silver ‚Üí Gold):

```mermaid
graph LR
    A["üì¶ Landing Zone<br>CSVs Raw"] -->|"Ingesta Incremental<br>MultiLine Fix"| B["(Bronze Layer<br>Parquet)"]
    B -->|"Limpieza, Tipado<br>& Business Logic"| C["(Silver Layer<br>Delta Tables)"]
    C -->|"Modelado Dimensional<br>Fact & Dims"| D["(Gold Layer<br>Star Schema)"]
    D -->|Consumo| E["üìä BI & Analytics<br>PowerBI / SQL"]
```

## üìÇ Estructura del Proyecto

```
/ModernELT-Brazilian-Ecommerce
‚îú‚îÄ‚îÄ 00_init.py                    # Inicializaci√≥n de directorios (Landing, Bronze, Silver, Gold)
‚îú‚îÄ‚îÄ 01_download_landing.py        # Descarga autom√°tica de Kaggle -> Landing Zone
‚îú‚îÄ‚îÄ 02_process_bronze.py          # Ingesta Raw -> Parquet (Manejo de CSVs complejos + Manifest)
‚îú‚îÄ‚îÄ 03_silver_business_cases.py   # Transformaci√≥n, Limpieza y L√≥gica de Negocio (SCD Type 1)
‚îú‚îÄ‚îÄ 04_Process_Gold_Layer.py      # Modelado Dimensional (Star Schema) y Optimizaci√≥n Z-Order
‚îî‚îÄ‚îÄ utils.py                      # Configuraci√≥n de entidades y utilitarios compartidos
```

## üöÄ Caracter√≠sticas T√©cnicas (Key Features)

### 1. Ingesta Robusta (Bronze)

**Desaf√≠o:** El dataset de reviews conten√≠a saltos de l√≠nea (`\n`) dentro de los comentarios, lo que romp√≠a la estructura del CSV y generaba "Column Shifts".

**Soluci√≥n:** Implementaci√≥n de lectura con `multiLine=True` y normalizaci√≥n de nombres de columnas.

**Formato:** Parquet (compresi√≥n Snappy).

### 2. L√≥gica de Negocio & Calidad (Silver)

- **Framework de Upsert:** Funci√≥n gen√©rica para aplicar MERGE (SCD Tipo 1) en Delta Lake, garantizando idempotencia.
- **Defensive Coding:** Uso de `try_cast` para manejar datos sucios sin detener el pipeline (Null Safety).
- **Enriquecimiento:**
  - C√°lculo de `lead_time_days` (Tiempo de entrega).
  - Flags anal√≠ticos: `is_on_time_delivery`, `is_high_ticket`, `sentiment`.
  - Deduplicaci√≥n de geolocalizaci√≥n (Centroides por Zip Code).

### 3. Modelado Anal√≠tico Avanzado (Gold)

**Esquema de Estrella:** Transformaci√≥n de 8 tablas relacionales a un modelo optimizado para BI.

- **Facts:** `fact_sales` (Transaccional), `fact_logistics` (Snapshot de estado).
- **Dims:** `dim_customer`, `dim_product`, `dim_seller`, `dim_date`.

**Performance Tuning:**
- **Particionamiento:** Por `date_key` para poda de archivos (Pruning).
- **Z-Ordering:** Co-localizaci√≥n f√≠sica de datos (`product_id`, `customer_id`) para Data Skipping masivo.
- **Vacuum:** Limpieza de versiones antiguas para optimizaci√≥n de almacenamiento.

## üóÑÔ∏è Esquemas de Datos (Databases)

| Schema | Prop√≥sito | Tablas |
|--------|-----------|--------|
| `olist_meta` | Metadatos y control de pipeline | `bronze_manifest` |
| `olist_silver` | Datos limpios y normalizados | `orders`, `order_items`, `products`, `customers`, `sellers`, `geolocation`, `order_reviews`, `order_payments` |
| `olist_gold` | Modelo dimensional anal√≠tico | `fact_sales`, `fact_logistics`, `dim_customer`, `dim_product`, `dim_seller`, `dim_date` |

## üìä Modelo de Datos (Gold Layer)

El dise√±o final permite responder preguntas complejas de negocio mediante SQL simple.

| Tabla | Tipo | Descripci√≥n | Granularidad |
|-------|------|-------------|--------------|
| `fact_sales` | Fact | Ventas, ingresos y fletes. | Item de Orden |
| `fact_logistics` | Fact | Tiempos de entrega, retrasos y SLA. | Orden |
| `dim_customer` | Dim | Datos demogr√°ficos y ubicaci√≥n (Geo). | Cliente |
| `dim_product` | Dim | Categor√≠a, volumen (cm3) y peso. | Producto |
| `dim_date` | Dim | Calendario fiscal y atributos temporales. | D√≠a |

## üìà Insights de Negocio (Ejemplos SQL)

Una vez ejecutado el pipeline, podemos responder preguntas clave:

### 1. Ingresos Mensuales

```sql
SELECT d.year, d.month, FORMAT_NUMBER(SUM(f.total_amount), 2) as revenue
FROM olist_gold.fact_sales f
JOIN olist_gold.dim_date d ON f.date_key = d.date_key
GROUP BY 1, 2 ORDER BY 1 DESC, 2 DESC;
```

### 2. An√°lisis de Retrasos (SLA)

```sql
SELECT 
    delivery_status_detail,
    speed_class,
    COUNT(order_id) as order_count,
    ROUND(AVG(CASE WHEN delay_days > 0 THEN delay_days ELSE NULL END), 1) as avg_delay_days
FROM olist_gold.fact_logistics
WHERE date_key >= '2017-01-01'
GROUP BY 1, 2
ORDER BY 3 DESC;
```

### 3. Top 10 Categor√≠as por GMV

```sql
SELECT 
    p.category,
    SUM(f.sales_amount) as gmv,
    ROUND(AVG(f.sales_amount), 2) as avg_ticket
FROM olist_gold.fact_sales f
JOIN olist_gold.dim_product p ON f.product_key = p.product_key
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10;
```

### 4. Revenue por Estado

```sql
SELECT 
    c.customer_state,
    COUNT(DISTINCT f.order_id) as num_orders,
    FORMAT_NUMBER(SUM(f.total_amount), 2) as total_revenue
FROM olist_gold.fact_sales f
JOIN olist_gold.dim_customer c ON f.customer_key = c.customer_key
GROUP BY 1
ORDER BY 3 DESC;
```

### 5. Compras Premium en Fin de Semana

```sql
SELECT 
    o.order_id,
    c.customer_city,
    f.total_amount,
    d.day_name
FROM olist_gold.fact_sales f
JOIN olist_gold.dim_date d ON f.date_key = d.date_key
JOIN olist_gold.dim_customer c ON f.customer_key = c.customer_key
JOIN olist_silver.orders o ON f.order_id = o.order_id
WHERE 
    f.is_high_ticket = true 
    AND d.is_weekend = true
ORDER BY f.total_amount DESC
LIMIT 20;
```

## üõ†Ô∏è C√≥mo Ejecutar este Proyecto

### Requisitos

- Cuenta de Databricks (Community/Free Edition sirve).
- Cluster con Spark 3.4+ y Delta Lake.
- Credenciales de Kaggle (API Token `kaggle.json`).
- Unity Catalog configurado con Volumes.

### Pasos

#### 1. **Configuraci√≥n Inicial**
   
Ejecuta `00_init.py` para crear la estructura de directorios:
```python
# Par√°metro: path base del proyecto
# Crea: /landing, /bronze, /silver, /gold, /_checkpoints, /_meta
```

#### 2. **Descarga Autom√°tica de Datos (Landing Zone)**
   
Ejecuta `01_download_landing.py`:
```python
# Par√°metros:
# - run_date: fecha de ejecuci√≥n (default: hoy)
# - force_download: true/false (re-descarga si ya existe)

# Descarga dataset de Kaggle usando API
# Output: /Volumes/workspace/default/data/olist/landing/YYYY-MM-DD/
```

**Caracter√≠sticas**:
- Integraci√≥n con Kaggle API (autenticaci√≥n v√≠a `kaggle.json`).
- Control de duplicados: no re-descarga si ya existe (configurable).
- Organizaci√≥n por fecha de ejecuci√≥n.

#### 3. **Procesamiento Bronze (Ingesta Incremental)**
   
Ejecuta `02_process_bronze.py`:
```python
# Lee CSVs desde Landing Zone
# Aplica multiLine=True para manejar saltos de l√≠nea en reviews
# Escribe en formato Parquet particionado por run_date
```

**Caracter√≠sticas Avanzadas**:
- **Manifest Tracking**: Tabla Delta `olist_meta.bronze_manifest` registra archivos procesados (nombre, size, timestamp).
- **Procesamiento Incremental**: Solo procesa archivos nuevos usando `LEFT ANTI JOIN` contra el manifest.
- **Idempotencia**: Re-ejecutar con mismos datos no genera duplicados.
- **Normalizaci√≥n**: Columnas en lowercase y sin espacios.

#### 4. **Transformaci√≥n Silver (Business Logic)**
   
Ejecuta `03_silver_business_cases.py`:

**Transformaciones Implementadas**:

| Entidad | L√≥gica de Negocio | Ejemplo |
|---------|-------------------|---------|
| **orders** | C√°lculo de `lead_time_days`, `delay_days`, flags de on-time delivery | `is_on_time_delivery = delivered_ts <= estimated_ts` |
| **products** | C√°lculo de volumen (cm¬≥), clasificaci√≥n por peso | `volume_cm3 = length √ó height √ó width` |
| **reviews** | Categorizaci√≥n de sentimiento basada en score | `sentiment = CASE score >= 4 THEN 'Positive'` |
| **geolocation** | Deduplicaci√≥n por zip code usando centroides | `groupBy(zip).agg(first(lat), first(lng))` |
| **payments** | Flag de compras a plazos | `is_installment = installments > 1` |

**Framework de Upsert**:
- Funci√≥n gen√©rica `upsert_to_silver()` para aplicar MERGE (SCD Type 1).
- Deduplicaci√≥n autom√°tica por Primary Keys.
- Manejo de errores con `try_cast` para datos corruptos.

#### 5. **Modelado Gold (Star Schema)**
   
Ejecuta `04_Process_Gold_Layer.py`:

**Dimensiones Creadas**:
- `dim_date`: Calendario completo 2016-2025 con atributos temporales (weekend, holiday season).
- `dim_customer`: Enriquecida con coordenadas geogr√°ficas (join con geolocation).
- `dim_product`: Incluye volumen, peso y clasificaci√≥n.
- `dim_seller`: Datos demogr√°ficos + lat/lng.

**Tablas de Hechos**:
- **`fact_sales`**: Granularidad item-level, particionada por `date_key`.
  - M√©tricas: `sales_amount`, `freight_amount`, `total_amount`.
  - Flags: `is_high_ticket` (> $500).
  
- **`fact_logistics`**: Granularidad order-level, an√°lisis SLA.
  - M√©tricas: `lead_time_days`, `delay_days`.
  - Dimensiones derivadas: `delivery_status_detail`, `speed_class`.

**Optimizaciones Aplicadas**:
```python
# Particionamiento para File Pruning
.partitionBy("date_key")

# Z-Ordering (ejecutar post-carga inicial)
OPTIMIZE olist_gold.fact_sales ZORDER BY (product_key, customer_key);
OPTIMIZE olist_gold.fact_logistics ZORDER BY (customer_key);

# Limpieza de versiones antiguas
VACUUM olist_gold.fact_sales RETAIN 168 HOURS;
```

## ‚ö° Mejores Pr√°cticas Implementadas

### 1. **Procesamiento Incremental**
- Sistema de **Manifest Tracking** (`olist_meta.bronze_manifest`) que registra archivos procesados.
- Detecci√≥n autom√°tica de nuevos archivos mediante `LEFT ANTI JOIN`.
- Evita reprocesamiento innecesario de datos ya cargados.

### 2. **Idempotencia**
- Framework de **Upsert gen√©rico** usando Delta MERGE.
- Re-ejecutar el pipeline con los mismos datos no genera duplicados.
- SCD Type 1: actualiza registros existentes, inserta nuevos.

### 3. **Defensive Coding**
- Uso de `coalesce` y casting defensivo para manejar datos nulos/corruptos.
- Validaci√≥n de esquemas con `multiLine=True` para CSVs complejos.
- Normalizaci√≥n autom√°tica de nombres de columnas (lowercase, sin espacios).

### 4. **Optimizaci√≥n de Performance**
```python
# Shuffle Partitions optimizado
spark.conf.set("spark.sql.shuffle.partitions", "8")

# Particionamiento f√≠sico
.partitionBy("date_key")

# Data Skipping con Z-Order
OPTIMIZE table_name ZORDER BY (high_cardinality_cols);

# Limpieza de versiones antiguas
VACUUM table_name RETAIN 168 HOURS;
```

### 5. **Organizaci√≥n de Datos**
- **Arquitectura Medallion**: Separaci√≥n clara entre Raw, Curated y Analytics.
- **Unity Catalog**: Uso de Volumes para almacenamiento estructurado.
- **Nombrado Consistente**: Convenci√≥n de nombres clara (`date_key`, `customer_key`, etc.).

### 6. **L√≥gica de Negocio Centralizada**
- Transformaciones en Silver para reutilizaci√≥n en Gold.
- C√°lculos complejos encapsulados (lead_time, sentiment, volume).
- Flags anal√≠ticos pre-calculados para acelerar queries.

## üì¶ Entidades del Dataset (9 Tablas)

| Entidad | Descripci√≥n | Registros (aprox.) |
|---------|-------------|-------------------|
| `customers` | Clientes √∫nicos | ~99K |
| `orders` | √ìrdenes de compra | ~99K |
| `order_items` | Items individuales por orden | ~112K |
| `order_payments` | M√©todos de pago | ~103K |
| `order_reviews` | Reviews y ratings | ~99K |
| `products` | Cat√°logo de productos | ~32K |
| `sellers` | Vendedores (marketplace) | ~3K |
| `geolocation` | Coordenadas de zip codes | ~1M |
| `product_category_name_translation` | Traducci√≥n PT‚ÜíEN | ~71 |

## üë§ Autor

**John Kl**  
Data Engineer en formaci√≥n  
[LinkedIn](https://www.linkedin.com/in/john-luis-alberto-castillo-reupo-aa3125253/) | [GitHub](https://github.com/Johnkl72)

---

*Este proyecto fue dise√±ado siguiendo principios de Clean Code y Arquitectura Escalable.*
